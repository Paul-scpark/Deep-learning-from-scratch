{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a738a3",
   "metadata": {},
   "source": [
    "# <font color=red>밑바닥부터 시작하는 딥러닝 1 - 한빛미디어, 사이토 고키</font>\n",
    "\n",
    "### Contents (Chapter)\n",
    "\n",
    "1. 헬로 파이썬 - 파이썬 기초 문법 소개, numpy, matplotlib\n",
    "2. 퍼셉트론 - AND, NAND, OR 게이트\n",
    "3. 신경망 - 활성화 함수, 다차원 배열 계산, 출력층 설계, MNIST\n",
    "4. **`신경망 학습 - 손실 함수, 경사 하강법`**\n",
    "5. 오차역전파법 - 역전파, 활성화 함수 구현\n",
    "6. 학습 관련 기술들 - 매개변수 갱신, 배치 정규화, 하이퍼파라미터 값 찾기\n",
    "7. 합성곱 신경망 (CNN) - 합성곱 계층, 풀링 계층, CNN 구현\n",
    "8. 딥러닝 - 초기 역사, 딥러닝 활용\n",
    "9. Appendix - Softmax with loss 계층의 계산 그래프\n",
    "\n",
    "### 참고 자료\n",
    "\n",
    "- [책 소개 링크](http://www.yes24.com/Product/Goods/34970929)\n",
    "- [책 깃헙 링크](https://github.com/WegraLee/deep-learning-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b248ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23887e",
   "metadata": {},
   "source": [
    "## <font color=orange>Chapter 4. 신경망학습</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be758572",
   "metadata": {},
   "source": [
    "### 4.1 데이터에서 학습한다!\n",
    "\n",
    "- 신경망의 특징은 데이터를 보고, 학습 할 수 있다는 점\n",
    "- **학습**이란, 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\n",
    "- 학습의 목표는 **손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것**\n",
    "\n",
    "#### 4.1.1 데이터 주도 학습\n",
    "\n",
    "- **기계학습**은 데이터에서 답을 찾고, 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것\n",
    "- 따라서 기계학습의 중심에는 **데이터**가 존재한다고 할 수 있음\n",
    "- 기계학습에서는 `사람의 개입을 최소화`하면서 수집한 데이터로 패턴을 찾으려 시도\n",
    "- MNIST 모델에서 숫자를 인식하는 알고리즘이 동작하기 위해서,\n",
    "    - 이미지에서 특징 (Feature)을 추출하여 그 특징의 패턴을 기계학습 기술로 학습\n",
    "    - **특징**은 입력 데이터에서 본질적인 데이터를 정확히 추출할 수 있도록 설계된 변환기\n",
    "    - 이미지의 특징은 일반적으로 '벡터'로 변환하여 학습\n",
    "- 기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 **기계**가 한다고 할 수 있음\n",
    "- 사람이 직접 설계하는 것에 비해서 부담은 적지만, 이미지를 벡터로 변환하는 등의 과정은 사람이 해야 함\n",
    "- 문제에 적합한 특징을 설계하지 (전처리) 못한다면, 좋은 결과를 획득하기도 어렵다고 할 수 있음\n",
    "- 딥러닝을 입력부터 출력까지 사람의 개입 없이 동작하여 **종단간 기계학습 (end-to-end machine learning)**이라고도 함\n",
    "\n",
    "#### 4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    "- 기계학습 문제는 **훈련 데이터 (Training data)와 시험 데이터 (Test data)**로 나눠 학습과 실험을 수행\n",
    "- 훈련 데이터를 통해서 학습하면서 최적의 매개변수를 찾음\n",
    "- 그 후에 시험 데이터를 사용하여 앞서 훈련한 모델의 성과를 평가\n",
    "- 결국 모델링의 궁극적인 목적은 범용적으로 사용할 수 있는 **일반화된 모델**이므로, 훈련 데이터와 시험 데이터로 구분\n",
    "- 이를 위해서 한번도 보지 못했던 훈련에 포함되지 않은 데이터로 성능을 측정\n",
    "- 한 데이터셋에만 지나치게 최적화 된 상태를 **오버피팅 (Overfitting)**이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df71f22",
   "metadata": {},
   "source": [
    "### 4.2 손실 함수\n",
    "\n",
    "- 신경망 학습에서는 현재의 상태를 `하나의 지표`로 표현\n",
    "- 그 지표를 가장 좋게 만들어주는 가중치 매개변수 값을 찾는 것이 목적\n",
    "- 신경망 학습에서 사용하는 지표는 **손실 함수 (Loss function)**이라고 하는데, 일반적으로 `오차제곱합과 교차 엔트로피 오차`를 사용\n",
    "    - 손실 함수는 신경망의 성능의 나쁨을 나타내는 지표\n",
    "    - 즉, 현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지의 성능을 담고 있음\n",
    "    - 따라서 손실 함수의 값이 클수록 좋지 않는 성능을 갖고 있다고 할 수 있음\n",
    "    \n",
    "#### 4.2.1 오차제곱합 (Sum of Square for Error, SSE)\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}{\\sum_{k} (y_k - t_k)^2}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 가장 많이 쓰이는 손실 함수는 **오차제곱합**이고, 수식은 위와 같음\n",
    "- $y_k$는 신경망의 출력 (신경망이 출력한 값), $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타냄\n",
    "- 즉, 오차제곱합은 각 원소의 출력과 정답 레이블의 차를 제곱한 후에 그 총합을 구하는 것\n",
    "- 파이썬 코드로 오차제곱합을 구현하면, 아래와 같음을 알 수 있음\n",
    "    - MNIST에서 정답이 '2'라고 했을 때, 모델의 출력 결과가 맞았을 때와 틀렸을 때의 손실함수 값 확인\n",
    "    - 출력 결과가 맞을 때, 손실 함수 값이 0.0975로 작게 나오는 것을 확인할 수 있음\n",
    "    - 반면, 출력 결과가 틀렸을 때, 손실 함수 값이 0.5975로 크게 나오는 것을 확인할 수 있음\n",
    "    - 즉, `손실 함수의 값이 작을수록 정답에 수렴한다`고 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503f686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "# 정답은 '2'\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 예1: '2'일 확률이 가장 높을 때\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t)) # 0.09750000000000003\n",
    "\n",
    "# 예2: '7'일 확률이 가장 높을 때\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t)) # 0.5975"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18e753",
   "metadata": {},
   "source": [
    "#### 4.2.2 교차 엔트로피 오차 (Cross Entropy Error, CEE)\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "E = -{\\sum_{k} t_klog_ey_k}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 위 수식은 교차 엔트로피 오차의 수식\n",
    "- log는 밑이 e인 자연로그, $y_k$는 신경망의 출력, $t_k$는 정답 레이블이면서 정답 인덱스만 1 (원-핫 인코딩)\n",
    "- 따라서 실질적으로 정답일 때 추정의 자연로그를 계산하는 식 (다른 경우에는 모두 0이기 때문)\n",
    "- 즉, `교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 결정`\n",
    "- 파이썬 코드로 교차 엔트로피 오차를 구현하면, 아래와 같음을 알 수 있음\n",
    "    - MNIST에서 정답이 '2'라고 했을 때, 모델의 출력 결과가 맞았을 때와 틀렸을 때의 손실함수 값 확인\n",
    "    - 출력 결과가 맞을 때, 손실 함수 값이 0.5108로 작게 나오는 것을 확인할 수 있음\n",
    "    - 반면, 출력 결과가 틀렸을 때, 손실 함수 값이 2.3025로 크게 나오는 것을 확인할 수 있음\n",
    "    - 즉, 위의 오차제곱합과 동일하게 `손실 함수의 값이 작을수록 정답에 수렴한다`고 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10158e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025840929945454"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# 정답은 '2'\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 예1: '2'일 확률이 가장 높을 때\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 0.510825457099338\n",
    "\n",
    "# # 예2: '7'일 확률이 가장 높을 때\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t)) # 2.3025840929945454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b4220",
   "metadata": {},
   "source": [
    "#### 4.2.3 미니배치 학습\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "E = -\\frac{1}{N}{\\sum_{n}}{\\sum_{k} t_klog_ey_k}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 위 수식은 **훈련 데이터 모두**에 대한 손실 함수 값을 구하는 수식\n",
    "- 앞선 수식과 비슷하고, 데이터 하나에 대한 손실 함수를 단순히 N개의 데이터로 확장한 것\n",
    "- 마지막에는 N으로 나눠서 정규화 (평균 손실 함수의 역할)\n",
    "- 하지만 현실적으로 **빅데이터 안에서 이 모든 데이터를 대상으로 값을 계산하는 것은 비효율적**\n",
    "- 따라서 데이터 일부를 추려 전체의 근사치로 이용하는 **미니배치 (Mini-batch)** 방법을 사용 (일부만 골라 학습)\n",
    "- 아래 코드는 MNIST 데이터셋에서 np.random.choice 함수를 통해 미니배치로 계산하는 과정을 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e38b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "github_url = '/Users/paul/Desktop/github/deep-learning-from-scratch-master/'\n",
    "sys.path.append(github_url)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "### MNIST 데이터셋 불러오기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "x_train.shape # (60000, 784)\n",
    "t_train.shape # (60000, 10)\n",
    "\n",
    "### 훈련 데이터에서 무작위로 10장만 추출하기\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866d9d0",
   "metadata": {},
   "source": [
    "#### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f231043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "#     return -np.sum(t * np.log(y + 1e-7)) / batch_size # 원-핫 인코딩이 되어 있는 경우\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size # 원-핫 인코딩이 되어 있지 않은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fb90a",
   "metadata": {},
   "source": [
    "- 배치 데이터를 지원하는 교차 엔트로피 오차 구현\n",
    "- y는 신경망의 출력, t는 정답 레이블\n",
    "- 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d9e7d",
   "metadata": {},
   "source": [
    "#### 4.2.5 왜 손실 함수를 설정하는가?\n",
    "\n",
    "- 모델의 궁극적인 목적은 높은 정확도를 끌어내는 매개변수 값을 찾아내는 것\n",
    "- 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾음\n",
    "- 이때 매개변수의 미분 (기울기)을 계산하여, 그 값을 단서로 값을 서서히 갱신하는 과정을 반복\n",
    "- 가중치 매개변수의 손실 함수 미분이라는 것은 **가중치 매개변수 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하는지**에 대한 것\n",
    "    - 미분 값이 음수면, 그 가중치 매개변수를 양의 방향으로 변화시켜서 손실 함수의 값을 줄일 수 있음\n",
    "    - 반대로 미분 값이 양수면, 가중치 매개변수를 음의 방향으로 변화시켜서 손실 함수 값을 줄일 수 있음\n",
    "    - 하지만 미분 값이 0이면, 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수 값은 변화하지 않으므로, 매개변수 갱신은 중단\n",
    "- 정확도를 지표로 삼게 되면, 결과값이 연속적으로 변화하지 못하고, 불연속적으로 띄엄띄엄한 값으로 변화\n",
    "- 한편, 손실 함수를 지표로 삼으면, 매개변수 값이 변할 때마다 그에 반응하여 손실 함수도 연속적으로 값이 변화\n",
    "- 따라서 활성화 함수로 계단 함수를 사용하지 않는 이유 역시, 미분값이 불연속적으로 계산되기 때문임\n",
    "- 시그모이드 함수 같은 경우에는, 출력이 연속적으로 변하면서 곡선의 기울기 역시 연속적으로 변화함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e39a77",
   "metadata": {},
   "source": [
    "### 4.3 수치 미분\n",
    "\n",
    "- 경사법에서는 기울기 (경사) 값을 기준으로 나아갈 방향을 정함\n",
    "\n",
    "#### 4.3.1 미분\n",
    "\n",
    "- **미분**은 한순간의 변화량을 표시한 것, **수치 미분**은 아주 작은 차분으로 미분하는 것\n",
    "- 미분을 수식으로 표현하면 아래와 같은데, 이 뜻은 x의 작은 변화가 함수 $f(x)$를 얼마나 변화시키느냐에 대한 것\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31574ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eea97",
   "metadata": {},
   "source": [
    "#### 4.3.2 수치 미분의 예\n",
    "\n",
    "- $y = 0.01x^2 + 0.1x$를 그래프로 표현하면 아래와 같음\n",
    "- 해당 식에서 x가 5일 때와 10일 때의 미분 결과는 각각 0.2, 0.3 정도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d9a13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1) # 0에서 20까지 0.1 간격의 배열 x를 만듦\n",
    "y = function_1(x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "numerical_diff(function_1, 5)  # 0.1999999999990898\n",
    "numerical_diff(function_1, 10) # 0.2999999999986347"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a1fc6",
   "metadata": {},
   "source": [
    "#### 4.3.3 편미분\n",
    "\n",
    "- $f(x_0, x_1) = x_0^2 + x_1^2$ 라는 식을 파이썬으로 구현하면 아래와 같음\n",
    "- 이 식을 미분하려고 할때, 주의해야 하는 것은 변수가 2개이므로, 어느 변수에 대한 미분인지가 중요\n",
    "- 이처럼 변수가 여럿인 함수에 대한 미분을 **편미분**이라고 정의\n",
    "- 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구하는 것\n",
    "- 단, 여러 변수들 중에서 목표 변수 하나에 초점을 맞추고, 다른 변수는 값을 고정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b9fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4e074",
   "metadata": {},
   "source": [
    "### 4.4 기울기 (Gradient)\n",
    "\n",
    "- 모든 변수의 편미분을 벡터로 정리한 것\n",
    "- 아래 그림은 $x_0^2 + x_1^2$의 기울기를 나타내는 그림\n",
    "- 아래 그림에서 확인할 수 있듯, 기울기는 함수의 **가장 낮은 장소 (최솟값)**를 가리키는 것 같은 모양\n",
    "- 가장 낮은 곳에서 멀어질수록 화살표의 크기가 커짐\n",
    "- 하지만 정확하게 말한다면, 기울기는 각 지점에서 낮아지는 방향을 가리킴\n",
    "    - 즉, 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향\n",
    "    - Global Optima, Local Optima 개념 참고\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbADIrW%2Fbtra6IQukSn%2Fu5FHuUcArKf3FKp5V6skik%2Fimg.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efea12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        # f(x + h) 계산\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x - h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "    return grad\n",
    "\n",
    "numerical_gradient(function_2, np.array([3.0, 4.0])) # array([6., 8.])\n",
    "numerical_gradient(function_2, np.array([0.0, 2.0])) # array([0., 4.])\n",
    "numerical_gradient(function_2, np.array([3.0, 0.0])) # array([6., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab76ff",
   "metadata": {},
   "source": [
    "#### 4.4.1 경사법 (경사 하강법)\n",
    "\n",
    "- 기계학습에서는 학습 단계에서 최적의 매개변수 (가중치와 편향)를 학습에서 찾음\n",
    "- 최적이라는 것은 손실함수가 최솟값이 될 때의 매개변수 값\n",
    "- 하지만 손실함수는 복잡하기 때문에, 어디가 최솟값인지 찾기 힘듦\n",
    "- 따라서 기울기를 잘 이용하여 함수의 (가능한 최대한 작은) 최솟값을 찾으려는 것이 **경사법**의 개념\n",
    "- 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 **기울기**\n",
    "    - 하지만 기울기가 가리키는 곳에 함수의 최솟값이 있는지 보장할 수는 없음\n",
    "    - 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없을 수도 있음\n",
    "- 그렇기 때문에 기울기 정보를 단서로 하여 나아갈 방향을 정하는 경사법 개념이 도입\n",
    "- 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한 후, 이동한 곳에서 기울기를 구하고, 이 과정을 반복\n",
    "- 이렇게 함수의 값을 점차 줄이는 것이 **경사법 (Gradient method)**이고, 기계학습을 최적화 하는데 사용\n",
    "    - 경사법에서 최솟값을 찾는 과정을 **경사 하강법 (Gradient descent method)**\n",
    "    - 경사법에서 최댓값을 찾는 과정을 **경사 상승법 (Gradient ascent method)**\n",
    "    \n",
    "<br/>\n",
    "\n",
    "$$\n",
    "x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 경사법을 수식으로 표현하면 위와 같음\n",
    "- $\\eta$는 갱신하는 양을 나타내고, 신경망 학습에서는 **학습률 (Learning rate)**이라고 정의\n",
    "- 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하는지를 정하는 것\n",
    "- 일반적으로 학습률은 0.01, 0.001 등의 값으로 정하는데, 이 값이 너무 크거나 작으면 좋은 장소로 찾아가기 힘듦\n",
    "- 학습률과 같은 매개변수를 **하이퍼파라미터 (Hyper parameter)**라고 정의\n",
    "    - 신경망의 가중치 매개변수는 훈련 데이터와 알고리즘으로 자동으로 획득되는 매개변수\n",
    "    - 하지만 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264aafe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34235971e+12, -3.96091057e+12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f = 최적화 하려는 함수, init_x = 초깃값, lr = 학습률, step_num = 경사법에 따른 반복 횟수\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "# 학습률에 따른 경사하강법의 결과\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)   # array([-6.11110793e-10,  8.14814391e-10])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)  # array([2.34235971e+12, -3.96091057e+12])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100) # array([-2.99999994,  3.99999992])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3fd0f",
   "metadata": {},
   "source": [
    "#### 4.4.2 신경망에서의 기울기\n",
    "\n",
    "- 기울기는 가중치 매개변수에 대한 손실 함수의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea397eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.46797384 -0.50796296 -0.11428422]\n",
      " [-0.83416617  0.39684863  1.41249757]]\n",
      "[-1.03153386  0.05238599  1.20267728]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3532035647660964"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "github_url = '/Users/paul/Desktop/github/deep-learning-from-scratch-master/'\n",
    "sys.path.append(github_url)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet: # x = 입력 데이터, t = 정답 레이블\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) # 정규분포로 초기화\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W)             # 가중치 매개변수\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)                 # [1.25184467 0.34731474 1.30469603]\n",
    "np.argmax(p)             # 최댓값의 인덱스 = 2\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "net.loss(x, t)           # 2.125447431676497 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ee20fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04512853  0.13341099 -0.17853952]\n",
      " [ 0.0676928   0.20011649 -0.26780928]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bc09d",
   "metadata": {},
   "source": [
    "### 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "- 전제: 신경망에는 적용 가능한 가중치와 편향이 있음. 이 값들을 훈련 데이터에 적응하도록 조정하는 것을 학습이라고 정의\n",
    "- 1단계: 미니배치\n",
    "    - 훈련 데이터 중 일부를 무작위로 가져오기\n",
    "    - 선별한 데이터를 미니배치라고 하고, 그 미니배치의 손실함수 값을 줄이는 것이 목표\n",
    "- 2단계: 기울기 산출\n",
    "    - 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기 구하기\n",
    "    - 기울기는 손실함수의 값을 가장 작게 하는 방향 제시\n",
    "- 3단계: 매개변수 갱신\n",
    "    - 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "- 4단계: 1~3단계를 반복\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 이 과정이 신경망 학습이 이뤄지는 순서. 경사하강법으로 매개변수를 갱신하는 방법\n",
    "- 이때 데이터를 미니배치로 무작위로 선정하므로, **확률적 경사 하강법 (Stochastic gradient descent, SGD)**로 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7d2b3",
   "metadata": {},
   "source": [
    "#### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3bde35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "github_url = '/Users/paul/Desktop/github/deep-learning-from-scratch-master/'\n",
    "sys.path.append(github_url)\n",
    "import numpy as np\n",
    "from common.functions import sigmoid, softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(x, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x = 입력 데이터, t = 정답 레이블\n",
    "    def loss(self, x, t): \n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads.params['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads.params['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads.params['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads.params['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape # (784, 100)\n",
    "net.params['b1'].shape # (100, )\n",
    "net.params['W2'].shape # (100, 10)\n",
    "net.params['b2'].shape # (10, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06149992",
   "metadata": {},
   "source": [
    "#### 4.5.2 미니배치 학습 구현하기\n",
    "\n",
    "- 미니배치 학습이란, 훈련 데이터 중 일부를 무작위로 꺼내고 (미니배치), 그 미니배치에 대해 경사법으로 매개변수를 갱신\n",
    "- 아래 예시에서는 미니배치 크기를 100으로 설정\n",
    "- 즉, 60,000개의 훈련 데이터에서 임의로 100개 데이터를 추리고, 이 데이터로 확률적 경사하강법을 수행해 매개변수를 갱신\n",
    "- 갱신 횟수 (반복 횟수)는 10,000번으로 설정하고, 갱신 할때마다 계산되는 손실 함수 값을 배열에 추가\n",
    "- 결과를 보면, 학습 횟수가 늘어날수록 손실 함수의 값이 줄어드는 것을 알 수 있음\n",
    "- 이 결과는 신경망의 가중치 매개변수가 서서히 데이터에 적응하면서 학습되고 있다는 것을 뜻함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a54aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1c65d",
   "metadata": {},
   "source": [
    "#### 4.5.3 시험 데이터로 평가하기\n",
    "\n",
    "- 앞서 훈련된 모델은 훈련 데이터에 대한 결과이므로, 다른 데이터셋에서도 동작하는지 확인이 필요함\n",
    "- 즉, 훈련 데이터에서만 학습한 결과인 오버피팅 되었는지를 확인해봐야 함\n",
    "- 신경망 학습의 목표는 범용적인 능력을 익히는 것 즉, 일반화라고 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7af05de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1 에폭 당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1 에폭 당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc: \" + str(train_acc) + ', ' + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8116e",
   "metadata": {},
   "source": [
    "### 4.6 정리\n",
    "\n",
    "- 손실함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수를 찾아내는 것이 신경망 학습의 목표\n",
    "- 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가\n",
    "- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치 값을 갱신하는 작업 반복"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
